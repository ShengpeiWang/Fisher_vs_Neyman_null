# Fisher_vs_Neyman_null
Simulate how Fisher's sharp null affect outcomes of t-test and permutation test

Please check out the jupyter notebooks for simulation code and visualizations. 

Statistical tests rely on testing between alternative hypotheses, which mean the construction of the null is the foundation of statistical testing. The was an interesting historical disagreement between two instrumental statisticians, Fisher and Neyman, in how to construct a good null hypothesis. Under Fisher's sharp null hypothesis of no treatment effect, each unit is assumed to have uniform response of exactly zero treatment effect. One the other hand, Neyman states that that's too stringent of an assumption, and the null hypothesis we are really interested in is that the population average response is zero. Based on my reading of their interactions, I think Fisher agrees with Neyman in terms of that being what we are really after. But using that as a null hypothesis will invalidate many of classical statistical methods, at least in principle. 

Thus, I conducted simulations to test what it means to generate a population of data if either null is correct. The simulations showed that significance of a statistic, either using a t-test or permutation test of mean difference, was not biased by whether the data was generated using the Fisher's sharp null hypothesis or Neyman's population average effect. A reason why these wasn’t an effect of the different null hypotheses was that I used t-test with independent variance.   

An interesting thing about the two hypotheses that I didn't find anyone discuss explicitly is that the data generation process is the same for the untreated group. The two hypotheses differ in how data is generated in the experiment group. Under Fisher's sharp null, the response can be partitioned between the average treatment effect, which is zero for all individuals, and the residual error. One the other hand, under Neyman's general null hypothesis, the response further includes the individual treatment effect, which differs among individuals and sum up to zero across the entire population. Because of the additional source of variance, the variance of the treated group is expected to be larger than the untreated group under Neyman’s null. This connection between Fisher’s sharp null and the common assumption of homogeneity of variance for linear models is very interesting. When I did research on linear models and t-tests, Fisher’s sharp null is generally not among the list of assumptions. On the other hand, the assumption of homogeneity in variance is very common. This comes back to when Fisher mention how his sharp null hypothesis is fundamental for z-test (which is the basis or very similar to many linear models), but his implication wasn’t clear to me until I did the simulations.

Please let me know if you have any thoughts or comments about this.
